{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "998947c6",
   "metadata": {},
   "source": [
    "# Model Evaluation and Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b43593c",
   "metadata": {},
   "source": [
    "## ðŸ“Œ Why Evaluate Models?\n",
    "\n",
    "Model evaluation ensures that our predictive model **generalizes well** to new data.  \n",
    "Key purposes:\n",
    "\n",
    "- Assess model **accuracy**\n",
    "- Compare **different models**\n",
    "- Detect **overfitting** or **underfitting**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb8f90a",
   "metadata": {},
   "source": [
    "## ðŸ“Œ Regression Metrics\n",
    "\n",
    "For continuous outcomes:\n",
    "\n",
    "1. **Mean Squared Error (MSE):**\n",
    "$$\n",
    "MSE = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "2. **Root Mean Squared Error (RMSE):**\n",
    "$$\n",
    "RMSE = \\sqrt{MSE}\n",
    "$$\n",
    "\n",
    "3. **Mean Absolute Error (MAE):**\n",
    "$$\n",
    "MAE = \\frac{1}{n} \\sum_{i=1}^n |y_i - \\hat{y}_i|\n",
    "$$\n",
    "\n",
    "4. **R-squared ($R^2$):** proportion of variance explained\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cee26ef",
   "metadata": {},
   "source": [
    "## ðŸ“Œ Classification Metrics\n",
    "\n",
    "For binary outcomes:\n",
    "\n",
    "- **Accuracy:** proportion of correctly classified instances  \n",
    "- **Precision:** proportion of positive predictions that are correct\n",
    "$$\n",
    "Precision = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "- **Recall (Sensitivity):** proportion of actual positives detected\n",
    "$$\n",
    "Recall = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "- **F1 Score:** harmonic mean of precision and recall\n",
    "$$\n",
    "F1 = 2 \\cdot \\frac{Precision \\cdot Recall}{Precision + Recall}\n",
    "$$\n",
    "- **ROC-AUC:** area under the Receiver Operating Characteristic curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dc1198",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Example: Evaluate a linear regression model\n",
    "set.seed(123)\n",
    "x <- 1:10\n",
    "y <- c(2.3, 3.1, 4.2, 5.0, 6.1, 7.2, 7.9, 8.8, 9.9, 10.2)\n",
    "\n",
    "# Fit linear model\n",
    "lm_model <- lm(y ~ x)\n",
    "\n",
    "# Predictions\n",
    "y_pred <- predict(lm_model)\n",
    "\n",
    "# Compute metrics\n",
    "MSE <- mean((y - y_pred)^2)\n",
    "RMSE <- sqrt(MSE)\n",
    "R2 <- 1 - sum((y - y_pred)^2) / sum((y - mean(y))^2)\n",
    "\n",
    "print(paste(\"RMSE:\", round(RMSE, 3)))\n",
    "print(paste(\"R-squared:\", round(R2, 3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7618cb78",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Example: Evaluate logistic regression\n",
    "set.seed(123)\n",
    "actual <- c(1, 0, 1, 1, 0, 0, 1, 0, 1, 0)\n",
    "pred_prob <- c(0.9, 0.1, 0.8, 0.7, 0.2, 0.3, 0.6, 0.4, 0.9, 0.2)\n",
    "pred_class <- ifelse(pred_prob > 0.5, 1, 0)\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix <- table(Predicted = pred_class, Actual = actual)\n",
    "print(conf_matrix)\n",
    "\n",
    "# Accuracy\n",
    "accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)\n",
    "print(paste(\"Accuracy:\", round(accuracy, 3)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2a831c",
   "metadata": {},
   "source": [
    "## ðŸ“Œ Comparing Models\n",
    "\n",
    "1. **Visual comparison:**\n",
    "   - Plot predicted vs actual values\n",
    "   - ROC curve for classification\n",
    "\n",
    "2. **Cross-validation:** Split data into **k folds** to test generalization\n",
    "\n",
    "3. **Information criteria for model selection:**\n",
    "   - **AIC (Akaike Information Criterion)**\n",
    "   - **BIC (Bayesian Information Criterion)**\n",
    "   - Lower values indicate better trade-off between fit and complexity\n",
    "\n",
    "4. **Nested models:** Compare simpler vs complex models using **likelihood ratio tests**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654e4ccb",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Fit two models\n",
    "lm_model1 <- lm(y ~ x)          # simple linear\n",
    "lm_model2 <- lm(y ~ x + I(x^2)) # quadratic model\n",
    "\n",
    "# Compare AIC and BIC\n",
    "AIC_model1 <- AIC(lm_model1)\n",
    "AIC_model2 <- AIC(lm_model2)\n",
    "BIC_model1 <- BIC(lm_model1)\n",
    "BIC_model2 <- BIC(lm_model2)\n",
    "\n",
    "print(paste(\"AIC Model 1:\", round(AIC_model1, 2)))\n",
    "print(paste(\"AIC Model 2:\", round(AIC_model2, 2)))\n",
    "print(paste(\"BIC Model 1:\", round(BIC_model1, 2)))\n",
    "print(paste(\"BIC Model 2:\", round(BIC_model2, 2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8139310",
   "metadata": {},
   "source": [
    "# Real-World Analogy\n",
    "\n",
    "- **Regression metrics:** Imagine predicting house prices. RMSE tells you the average error in dollars, and RÂ² tells you how well your model explains price variation.  \n",
    "- **Classification metrics:** Predicting if an email is spam. Accuracy shows overall correctness, but F1 balances detecting spam (recall) and avoiding false alarms (precision).  \n",
    "- **Model comparison:** Choosing between a simple linear model vs. a more complex polynomial model is like choosing a simple recipe vs. a fancy one â€” you want the best outcome without unnecessary complexity.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "name": "R"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
